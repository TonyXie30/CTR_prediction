{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model stage II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:45:12.776884600Z",
     "start_time": "2024-05-23T11:45:12.761223Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import xlearn as xl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:45:14.161230900Z",
     "start_time": "2024-05-23T11:45:14.144743600Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# read the config file\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "DATA_PATH = config['DATA_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:45:21.897650600Z",
     "start_time": "2024-05-23T11:45:15.887644600Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_FE = dd.read_csv(DATA_PATH+'tr_FE.csv').compute()\n",
    "features = dd.read_csv('feature.csv').compute()\n",
    "feature_columns = features.head(30)['feature'].tolist()\n",
    "\n",
    "X = tr_FE[feature_columns]\n",
    "y = tr_FE['click']\n",
    "\n",
    "X = X.astype({col: 'int32' for col in X.select_dtypes('bool').columns})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:45:22.515740700Z",
     "start_time": "2024-05-23T11:45:21.897650600Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:24:19.563139600Z",
     "start_time": "2024-05-23T11:24:19.547496600Z"
    }
   },
   "outputs": [],
   "source": [
    "skLR = LogisticRegression(max_iter=100,random_state=42)\n",
    "skLR.fit(X_train,y_train)\n",
    "# Predict the probabilities of the test set\n",
    "y_pred_proba = skLR.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "loss = log_loss(y_test,y_pred_proba)\n",
    "print(f'The AUC of the model is {auc}')\n",
    "print(f'The log loss is {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:24:19.578731900Z",
     "start_time": "2024-05-23T11:24:19.563139600Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:24:19.594369Z",
     "start_time": "2024-05-23T11:24:19.578731900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lightfm import LightFM\n",
    "from scipy.sparse import coo_matrix\n",
    "from lightfm.evaluation import auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:28:53.174977400Z",
     "start_time": "2024-05-23T11:28:08.053111100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.5576091966467143\n",
      "Log Loss:  1.2785696185925026\n"
     ]
    }
   ],
   "source": [
    "# 保存为 libsvm 格式\n",
    "X_train.to_csv('train.txt', sep=' ', index=False, header=False)\n",
    "X_test.to_csv('test.txt', sep=' ', index=False, header=False)\n",
    "y_train.to_csv('train_y.txt', sep=' ', index=False, header=False)\n",
    "y_test.to_csv('test_y.txt', sep=' ', index=False, header=False)\n",
    "\n",
    "# 创建 FM 模型\n",
    "fm_model = xl.create_fm()\n",
    "fm_model.setTrain('train.txt')\n",
    "fm_model.setValidate('test.txt')\n",
    "fm_model.setSigmoid()\n",
    "fm_model.fit(param={'task': 'binary', 'lr': 0.2, 'lambda': 0.002, 'metric': 'auc','epoch': 100}, \n",
    "             model_path='model.out')\n",
    "\n",
    "# 预测\n",
    "fm_model.setTest('test.txt')\n",
    "fm_model.predict('model.out', 'output.txt')\n",
    "\n",
    "# 读取预测结果并计算 AUC 和 Log Loss\n",
    "y_pred = pd.read_csv('output.txt', header=None)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "loss = log_loss(y_test, y_pred)\n",
    "print(\"AUC: \", auc)\n",
    "print(\"Log Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:25:58.618157500Z",
     "start_time": "2024-05-23T11:24:52.899591300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 1293726 samples, validate on 323432 samples, 5054 steps per epoch\n",
      "Epoch 1/10\n",
      "25s - loss:  0.4338 - auc:  0.6538 - val_auc:  0.6725\n",
      "Epoch 2/10\n",
      "26s - loss:  0.4250 - auc:  0.6803 - val_auc:  0.6903\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[46], line 33\u001B[0m\n\u001B[0;32m     30\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauc\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 33\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_model_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtarget\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# Predict the test data\u001B[39;00m\n\u001B[0;32m     36\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(test_model_input, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m)\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\DMP\\lib\\site-packages\\deepctr_torch\\models\\basemodel.py:262\u001B[0m, in \u001B[0;36mBaseModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, initial_epoch, validation_split, validation_data, shuffle, callbacks)\u001B[0m\n\u001B[0;32m    260\u001B[0m total_loss_epoch \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m total_loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m    261\u001B[0m total_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m--> 262\u001B[0m \u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    265\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m name, metric_fun \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\DMP\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     86\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[1;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\DMP\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[1;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\DMP\\lib\\site-packages\\torch\\optim\\adam.py:141\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    138\u001B[0m             \u001B[38;5;66;03m# record the step after step update\u001B[39;00m\n\u001B[0;32m    139\u001B[0m             state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m--> 141\u001B[0m     \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m           \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m           \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[43m           \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m           \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m           \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m           \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[43m           \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[43m           \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\DMP\\lib\\site-packages\\torch\\optim\\_functional.py:105\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    103\u001B[0m     denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(bias_correction2))\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 105\u001B[0m     denom \u001B[38;5;241m=\u001B[39m (\u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbias_correction2\u001B[49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[0;32m    109\u001B[0m step_size \u001B[38;5;241m=\u001B[39m lr \u001B[38;5;241m/\u001B[39m bias_correction1\n\u001B[0;32m    110\u001B[0m param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from deepctr_torch.inputs import get_feature_names, DenseFeat\n",
    "from deepctr_torch.models import DeepFM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Convert X and y into a DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "\n",
    "dense_features = df.columns.tolist()\n",
    "dense_features.remove('target')\n",
    "\n",
    "# Preprocessing\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "df[dense_features] = mms.fit_transform(df[dense_features])\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Generate feature columns\n",
    "feature_columns = [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "feature_names = get_feature_names(feature_columns)\n",
    "\n",
    "# Convert the data into model input\n",
    "train_model_input = {name: train[name] for name in feature_names}\n",
    "test_model_input = {name: test[name] for name in feature_names}\n",
    "\n",
    "# Create the model\n",
    "model = DeepFM(feature_columns, feature_columns,task='binary',\n",
    "                   l2_reg_embedding=1e-5, device=device)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['auc'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_model_input, train['target'].values, batch_size=256, epochs=10, verbose=2, validation_split=0.2)\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = model.predict(test_model_input, batch_size=256)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(test['target'].values, y_pred)\n",
    "print(\"AUC: \", auc)\n",
    "\n",
    "# Calculate log loss\n",
    "loss = log_loss(test['target'].values, y_pred)\n",
    "print(\"Log Loss: \", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide & Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:25:58.618157500Z",
     "start_time": "2024-05-23T11:25:58.618157500Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepctr_torch.models import WDL\n",
    "\n",
    "# Convert X and y into a DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "\n",
    "dense_features = df.columns.tolist()\n",
    "dense_features.remove('target')\n",
    "\n",
    "# Preprocessing\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "df[dense_features] = mms.fit_transform(df[dense_features])\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Generate feature columns\n",
    "feature_columns = [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "feature_names = get_feature_names(feature_columns)\n",
    "\n",
    "# Convert the data into model input\n",
    "train_model_input = {name: train[name] for name in feature_names}\n",
    "test_model_input = {name: test[name] for name in feature_names}\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = WDL(feature_columns, feature_columns,task='binary',\n",
    "                   l2_reg_embedding=1e-5, device=device)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['auc'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_model_input, train['target'].values, batch_size=256, epochs=10, verbose=2, validation_split=0.2)\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = model.predict(test_model_input, batch_size=256)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(test['target'].values, y_pred)\n",
    "print(\"AUC: \", auc)\n",
    "\n",
    "# Calculate log loss\n",
    "loss = log_loss(test['target'].values, y_pred)\n",
    "print(\"Log Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## lightgbm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 274523, number of negative: 1342635\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 989\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617158, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.169756 -> initscore=-1.587354\n",
      "[LightGBM] [Info] Start training from score -1.587354\n",
      "AUC:  0.7255826761432671\n",
      "Log Loss:  0.40886849827620014\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Train the model\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dtest = lgb.Dataset(X_test, label=y_test, reference=dtrain)\n",
    "param = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "num_round = 100\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dtrain, dtest])\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC: \", auc)\n",
    "\n",
    "# Calculate log loss\n",
    "loss = log_loss(y_test, y_pred)\n",
    "print(\"Log Loss: \", loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T11:34:53.128704100Z",
     "start_time": "2024-05-23T11:34:41.557758Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DCN（Deep & Cross Network）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 1293726 samples, validate on 323432 samples, 5054 steps per epoch\n",
      "Epoch 1/10\n",
      "28s - loss:  0.4323 - auc:  0.6598 - val_auc:  0.6704\n",
      "Epoch 2/10\n",
      "27s - loss:  0.4258 - auc:  0.6782 - val_auc:  0.6836\n",
      "Epoch 3/10\n",
      "27s - loss:  0.4223 - auc:  0.6889 - val_auc:  0.6900\n",
      "Epoch 4/10\n",
      "28s - loss:  0.4195 - auc:  0.6972 - val_auc:  0.6969\n",
      "Epoch 5/10\n",
      "36s - loss:  0.4177 - auc:  0.7020 - val_auc:  0.7009\n",
      "Epoch 6/10\n",
      "39s - loss:  0.4165 - auc:  0.7050 - val_auc:  0.7060\n",
      "Epoch 7/10\n",
      "54s - loss:  0.4155 - auc:  0.7077 - val_auc:  0.7088\n",
      "Epoch 8/10\n",
      "49s - loss:  0.4146 - auc:  0.7100 - val_auc:  0.7096\n",
      "Epoch 9/10\n",
      "32s - loss:  0.4139 - auc:  0.7119 - val_auc:  0.7125\n",
      "Epoch 10/10\n",
      "52s - loss:  0.4133 - auc:  0.7134 - val_auc:  0.7133\n",
      "AUC:  0.7144237708530117\n",
      "Log Loss:  0.41167094065908016\n"
     ]
    }
   ],
   "source": [
    "from deepctr_torch.inputs import get_feature_names, SparseFeat, DenseFeat\n",
    "from deepctr_torch.models import WDL, DCN, AutoInt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Convert X and y into a DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "\n",
    "dense_features = df.columns.tolist()\n",
    "dense_features.remove('target')\n",
    "\n",
    "# Preprocessing\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "df[dense_features] = mms.fit_transform(df[dense_features])\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Generate feature columns\n",
    "feature_columns = [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "feature_names = get_feature_names(feature_columns)\n",
    "\n",
    "# Convert the data into model input\n",
    "train_model_input = {name: train[name] for name in feature_names}\n",
    "test_model_input = {name: test[name] for name in feature_names}\n",
    "\n",
    "# Create the model\n",
    "# model = WDL(feature_columns, feature_columns, task='binary')\n",
    "model = DCN(feature_columns, feature_columns, task='binary')\n",
    "# model = AutoInt(feature_columns, feature_columns, task='binary')\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['auc'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_model_input, train['target'].values, batch_size=256, epochs=10, verbose=2, validation_split=0.2)\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = model.predict(test_model_input, batch_size=256)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(test['target'].values, y_pred)\n",
    "print(\"AUC: \", auc)\n",
    "\n",
    "# Calculate log loss\n",
    "loss = log_loss(test['target'].values, y_pred)\n",
    "print(\"Log Loss: \", loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T11:44:03.583354800Z",
     "start_time": "2024-05-23T11:37:39.587583800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[59], line 61\u001B[0m\n\u001B[0;32m     58\u001B[0m test_model_input \u001B[38;5;241m=\u001B[39m {name: test[name] \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m feature_names}\n\u001B[0;32m     60\u001B[0m \u001B[38;5;66;03m# 创建 AutoInt 模型\u001B[39;00m\n\u001B[1;32m---> 61\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoInt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlinear_feature_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_columns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdnn_feature_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_columns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbinary\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauc\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m# 训练 AutoInt 模型\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\DMP\\lib\\site-packages\\deepctr_torch\\models\\autoint.py:52\u001B[0m, in \u001B[0;36mAutoInt.__init__\u001B[1;34m(self, linear_feature_columns, dnn_feature_columns, att_layer_num, att_head_num, att_res, dnn_hidden_units, dnn_activation, l2_reg_dnn, l2_reg_embedding, dnn_use_bn, dnn_dropout, init_std, seed, task, device, gpus)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_dnn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(dnn_feature_columns) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(dnn_hidden_units) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     50\u001B[0m field_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_dict)\n\u001B[1;32m---> 52\u001B[0m embedding_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding_size\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(dnn_hidden_units) \u001B[38;5;129;01mand\u001B[39;00m att_layer_num \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     55\u001B[0m     dnn_linear_in_feature \u001B[38;5;241m=\u001B[39m dnn_hidden_units[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m field_num \u001B[38;5;241m*\u001B[39m embedding_size\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\DMP\\lib\\site-packages\\deepctr_torch\\models\\basemodel.py:527\u001B[0m, in \u001B[0;36mBaseModel.embedding_size\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    525\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(embedding_size_set) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    526\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding_dim of SparseFeat and VarlenSparseFeat must be same in this model!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 527\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43membedding_size_set\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T11:53:59.677371400Z",
     "start_time": "2024-05-23T11:53:53.664525600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
